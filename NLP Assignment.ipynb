{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "After cloning the github repository, I am importing the tweets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_pos = []\n",
    "labels_pos = []\n",
    "with open(\"pos_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        text_pos.append(i) \n",
    "        labels_pos.append('pos')\n",
    "\n",
    "text_neg = []\n",
    "labels_neg = []\n",
    "with open(\"neg_tweets.txt\") as f:\n",
    "    for i in f: \n",
    "        text_neg.append(i)\n",
    "        labels_neg.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a training and a testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_train, pos_test, pos_labels_train, pos_labels_test = train_test_split(text_pos, labels_pos, test_size=0.20)\n",
    "neg_train, neg_test, neg_labels_train, neg_labels_test = train_test_split(text_neg, labels_neg, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_tweets = pos_train + neg_train\n",
    "training_labels = pos_labels_train + neg_labels_train\n",
    "\n",
    "test_tweets = pos_test + neg_labels_test\n",
    "test_labels = pos_labels_test + neg_labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Important Feature list\n",
    "\n",
    "Converting the tweets as a tuple of list of words (removing punctuation and keeping lowercase) and sentiment.\n",
    "\n",
    "**Note: **  I am not removing stopwords, as the number of words in tweet are very low, and by removing them we can remove a significant portion of a tweet, which would hamper the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Function to remove Punctuation and keep everything in lower case\n",
    "def rem_Punct(sent):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_tokens = tokenizer.tokenize(sent)\n",
    "    return([w.lower() for w in word_tokens])\n",
    "    #return([w.lower() for w in word_tokens if not w.lower() in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "training_tweetslist = []\n",
    "for i, tweet in enumerate(training_tweets):\n",
    "    training_tweetslist.append((rem_Punct(tweet), training_labels[i]))\n",
    "\n",
    "# Testing\n",
    "test_tweetslist = []\n",
    "for i, tweet in enumerate(test_tweets):\n",
    "    test_tweetslist.append((rem_Punct(tweet), test_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting a list of all words in the all tweets of the training dataset\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We keep 500 most common words in the training dataset as Word Features\n",
    "allwords = Counter(get_words_in_tweets(training_tweetslist))\n",
    "word_features = [word_count[0] for word_count in allwords.most_common(500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see, these are the most common and useful words in tweets, therefore we use them to access the sentiment of each tweet !!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting the features list for each tweet\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, training_tweetslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = nltk.classify.apply_features(extract_features, test_tweetslist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have our training set, we will built a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      no = True              neg : pos    =     25.7 : 1.0\n",
      "                headache = True              neg : pos    =     18.2 : 1.0\n",
      "                 awesome = True              pos : neg    =     16.6 : 1.0\n",
      "                   thank = True              pos : neg    =     15.7 : 1.0\n",
      "                 excited = True              pos : neg    =     15.7 : 1.0\n",
      "                   great = True              pos : neg    =     14.2 : 1.0\n",
      "                  follow = True              pos : neg    =     14.2 : 1.0\n",
      "               beautiful = True              pos : neg    =     12.7 : 1.0\n",
      "                    love = True              pos : neg    =     11.2 : 1.0\n",
      "                    haha = True              pos : neg    =      9.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "example1 = \"Twilio is an awesome company!\"\n",
    "print(classifier.classify(extract_features(example1.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "example2 = \"I'm sad that Twilio doesn't have even more blog posts!\"\n",
    "print(classifier.classify(extract_features(example2.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "example3 = \"I have no headache!\"\n",
    "print(classifier.classify(extract_features(example3.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation :**\n",
    "As we can see, the model choose the tweet to be negative even though it is positive, this is presence of more negative word such as \"no\", \"headache\" which have a high neg:pos ratio . Therefore it is classified as 'Negative' as the model is unable to take into account the context and language used in the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Accuracy of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the test set: 0.917910447761194\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the test set: {}\".format(accuracy(classifier, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Testing on 10 random Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_pos_tweets = [\n",
    "    \"Coolest fan we’ve ever seen\",\n",
    "    \"sh in a good mood .....tlk to me\",\n",
    "    \"good morning I love you!!\",\n",
    "    \"Thanks, I need all the help i can get.\",\n",
    "    \"- @haugern The servers are now backup, if you experience any more problems then please let me know  Sorry about the delay...\"\n",
    "]\n",
    "    \n",
    "example_neg_tweets = [\n",
    "    'Donald Trump’s administration: “Government by the worst men.”',\n",
    "    'Their lies are not just lies. Their lies are authoritarian propaganda.',\n",
    "    \"or i just worry too much?\",\n",
    "    \"You're the only one who can see this cause no one else is following me this is for you because you're pretty awesome\",\n",
    "    \"Very sad about Iran.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coolest fan we’ve ever seen\n",
      "Classification: pos\n",
      "\n",
      "sh in a good mood .....tlk to me\n",
      "Classification: pos\n",
      "\n",
      "good morning I love you!!\n",
      "Classification: pos\n",
      "\n",
      "Thanks, I need all the help i can get.\n",
      "Classification: neg\n",
      "\n",
      "- @haugern The servers are now backup, if you experience any more problems then please let me know  Sorry about the delay...\n",
      "Classification: pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in example_pos_tweets:\n",
    "    print(tweet)\n",
    "    print(\"Classification: {}\".format(classifier.classify(extract_features(tweet.split()))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation :\n",
    "\n",
    "**As we can see, the model choose 1 tweet out of 5 (4th tweet) to be Negative even though all the tweets are positive, this is due to the words such as \"need\", \"help\" mostly these words are used in negative context, and the model does not account for the languages of the sentence therefore, it classified as negative.**\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump’s administration: “Government by the worst men.”\n",
      "Classification: neg\n",
      "\n",
      "Their lies are not just lies. Their lies are authoritarian propaganda.\n",
      "Classification: neg\n",
      "\n",
      "or i just worry too much?\n",
      "Classification: neg\n",
      "\n",
      "You're the only one who can see this cause no one else is following me this is for you because you're pretty awesome\n",
      "Classification: neg\n",
      "\n",
      "Very sad about Iran.\n",
      "Classification: neg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in example_neg_tweets:\n",
    "    print(tweet)\n",
    "    print(\"Classification: {}\".format(classifier.classify(extract_features(tweet.split()))))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note ::\n",
    "** Even through the nature of 4th tweet can be considered as positive, the model detects it as negative due to presence of more higher weightage words such as 'no' etc when compared to lower weightage positive words such as \"awesome\". Also, My model is able to detect all tweet correctly, which proves the model is working well, with high accuracy.**\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Improve Accuracy:\n",
    "\n",
    "1. Currently we have only removed punctuation, we can look at other NLP elements such as Stemming to filter words such as \"abend\", \"abending\", \"abended\" into one word \"abend\".\n",
    "2. Naive Bayes does not consider relationship between words, but other classifiers such as ensemble trees can be used to check if we can obtain better results.\n",
    "3. I am only keeping 500 most common words, but more words can be utilized to improve the model.\n",
    "4. Increasing the corpus to include more labelled tweets will help accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.twilio.com/blog/2017/09/sentiment-analysis-python-messy-data-nltk.html\n",
    "2. https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "3. https://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\n",
    "4. 10 random tweet examples :: https://www.kaggle.com/c/twitter-sentiment-analysis2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
